# This is an example .env file for the Model Context Reasoner (MCR).
# Copy this file to .env and fill in the necessary API keys and configurations.

# --- CHOOSE ONE LLM PROVIDER ---
# Uncomment and set the appropriate variables for your chosen provider.

# For OpenAI
# OPENAI_API_KEY="sk-..."
# MCR_LLM_MODEL_OPENAI="gpt-4o" # Optional, defaults to gpt-4o

# For Google Gemini
# GEMINI_API_KEY="..."
# MCR_LLM_MODEL_GEMINI="gemini-2.5-flash" # Optional, defaults to gemini-pro

# For local Ollama (no API key needed, but set the model if not using the default)
# MCR_LLM_PROVIDER="ollama"
# MCR_LLM_MODEL_OLLAMA="llama3" # Optional, defaults to llama3
# MCR_LLM_OLLAMA_BASE_URL="http://localhost:11434" # Optional, defaults to http://localhost:11434

    # For Generic OpenAI-compatible API (e.g. local LLM with OpenAI endpoint, custom proxy)
    # MCR_LLM_PROVIDER="generic_openai"
    # MCR_LLM_MODEL_GENERIC_OPENAI="your-model-name" # REQUIRED: Specify the model name for the generic endpoint
    # MCR_LLM_GENERIC_OPENAI_BASE_URL="http://localhost:8000/v1" # REQUIRED: Base URL of the OpenAI-compatible API
    # MCR_LLM_GENERIC_OPENAI_API_KEY="your_api_key_if_needed" # Optional: API key if the endpoint requires it

    # For Anthropic
    # MCR_LLM_PROVIDER="anthropic"
    # ANTHROPIC_API_KEY="sk-ant-..." # REQUIRED
    # MCR_LLM_MODEL_ANTHROPIC="claude-3-opus-20240229" # Optional, defaults to claude-3-opus-20240229

# --- OPTIONAL GENERAL SETTINGS ---
# MCR_API_URL="http://localhost:8080" # For CLI: defaults to http://localhost:8080 if server is local
# HOST="0.0.0.0" # Server host. Defaults to 0.0.0.0
# PORT="8080"    # Server port. Defaults to 8080
# LOG_LEVEL="info" # Logging level. Options: error, warn, info, http, verbose, debug, silly. Defaults to info.

# --- DATA STORAGE (Optional) ---
# Default paths are ./sessions_data and ./ontologies_data
# MCR_SESSION_STORAGE_PATH="./sessions_data"
# MCR_ONTOLOGY_STORAGE_PATH="./ontologies_data"
