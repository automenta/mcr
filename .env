# Model Context Reasoner (MCR) Environment Variables

# Server Configuration (Optional - defaults are provided)
# HOST=0.0.0.0
# PORT=8080

# Logging Configuration (Optional - defaults are provided)
# LOG_LEVEL=info # (e.g., error, warn, info, verbose, debug, silly)

# --- CHOOSE ONE LLM PROVIDER and configure accordingly ---
# Set MCR_LLM_PROVIDER to 'openai', 'gemini', or 'ollama'

# Option 1: OpenAI
# MCR_LLM_PROVIDER=openai
# OPENAI_API_KEY="sk-YOUR_OPENAI_API_KEY"
# MCR_LLM_MODEL_OPENAI="gpt-4o" # Optional, defaults to gpt-4o

# Option 2: Google Gemini
# MCR_LLM_PROVIDER=gemini
# GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
# MCR_LLM_MODEL_GEMINI="gemini-pro" # Optional, defaults to gemini-pro

# Option 3: Local Ollama
MCR_LLM_PROVIDER=ollama
# MCR_LLM_OLLAMA_BASE_URL="http://localhost:11434" # Optional, defaults to http://localhost:11434
# MCR_LLM_MODEL_OLLAMA="llama3" # Optional, defaults to llama3 (ensure this model is pulled in your Ollama instance)

# Note: If no MCR_LLM_PROVIDER is set, it defaults to 'openai'.
# The application will log warnings if required API keys for the selected provider are missing,
# but will still attempt to run. Functionality requiring the misconfigured LLM will be impaired.
