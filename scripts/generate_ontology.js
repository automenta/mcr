// scripts/generate_ontology.js
const fs = require('fs');
const path = require('path');
const yargs = require('yargs/yargs');
const { hideBin } = require('yargs/helpers');
const llmService = require('../src/llmService'); // Use imported service directly
const { prompts, fillTemplate } = require('../src/prompts'); // Adjust path
const logger = require('../src/logger'); // Adjust path

// Ensure the ontologies directory exists
const ontologiesDir = path.join(__dirname, '..', 'ontologies');
if (!fs.existsSync(ontologiesDir)) {
  fs.mkdirSync(ontologiesDir, { recursive: true });
}

async function generateOntology(
  domain,
  instructions,
  llmProviderName,
  modelName
) {
  logger.info(
    `Generating ontology for domain: "${domain}" with instructions: "${instructions}" using ${llmProviderName}`
  );

  if (!prompts.GENERATE_ONTOLOGY) {
    logger.error('GENERATE_ONTOLOGY prompt is not defined in prompts.js.');
    throw new Error('GENERATE_ONTOLOGY prompt is not defined.');
  }

  const originalProvider = process.env.MCR_LLM_PROVIDER;
  const originalOpenAIModel = process.env.MCR_LLM_MODEL_OPENAI;
  const originalGeminiModel = process.env.MCR_LLM_MODEL_GEMINI;
  const originalOllamaModel = process.env.MCR_LLM_MODEL_OLLAMA;

  if (llmProviderName) {
    logger.info(
      `Temporarily setting LLM provider to: ${llmProviderName} for this script run.`
    );
    process.env.MCR_LLM_PROVIDER = llmProviderName;
    if (modelName) {
      logger.info(
        `Temporarily setting model to: ${modelName} for ${llmProviderName}.`
      );
      if (llmProviderName.toLowerCase() === 'openai')
        process.env.MCR_LLM_MODEL_OPENAI = modelName;
      if (llmProviderName.toLowerCase() === 'gemini')
        process.env.MCR_LLM_MODEL_GEMINI = modelName;
      if (llmProviderName.toLowerCase() === 'ollama')
        process.env.MCR_LLM_MODEL_OLLAMA = modelName;
    }
    // Note: See generate_example.js for comments on limitations of this env var switching method
    // with module caching.
  }

  const filledPrompt = fillTemplate(prompts.GENERATE_ONTOLOGY.user, {
    domain,
    instructions,
  });
  const systemPrompt = prompts.GENERATE_ONTOLOGY.system;

  let generatedProlog;
  try {
    generatedProlog = await llmService.generate(systemPrompt, filledPrompt);
  } catch (error) {
    logger.error(`Error during LLM generation: ${error.message}`, error);
    throw error;
  } finally {
    // Restore original environment variables
    if (llmProviderName) {
      process.env.MCR_LLM_PROVIDER = originalProvider;
      if (modelName) {
        if (llmProviderName.toLowerCase() === 'openai')
          process.env.MCR_LLM_MODEL_OPENAI = originalOpenAIModel;
        if (llmProviderName.toLowerCase() === 'gemini')
          process.env.MCR_LLM_MODEL_GEMINI = originalGeminiModel;
        if (llmProviderName.toLowerCase() === 'ollama')
          process.env.MCR_LLM_MODEL_OLLAMA = originalOllamaModel;
      }
    }
  }

  logger.debug(`LLM Raw Output for ontology:\n${generatedProlog}`);

  // Clean up the Prolog output
  // Remove markdown code blocks if present
  let cleanedProlog = generatedProlog.replace(
    /```prolog\s*([\s\S]*?)\s*```/g,
    '$1'
  );
  cleanedProlog = cleanedProlog.replace(/```\s*([\s\S]*?)\s*```/g, '$1'); // Generic code block

  // Trim whitespace and ensure it's not empty
  cleanedProlog = cleanedProlog.trim();

  if (!cleanedProlog) {
    logger.warn('LLM generated empty Prolog content.');
    return;
  }

  // Add a comment header
  const commentHeader = `% Generated by scripts/generate_ontology.js for domain: ${domain}\n% Instructions: ${instructions}\n\n`;
  cleanedProlog = commentHeader + cleanedProlog;

  const fileName = `${domain.replace(/\s+/g, '_').toLowerCase()}GeneratedOntology.pl`;
  const filePath = path.join(ontologiesDir, fileName);

  fs.writeFileSync(filePath, cleanedProlog);
  logger.info(`Successfully generated ontology and saved to ${filePath}`);
}

if (require.main === module) {
  const argv = yargs(hideBin(process.argv))
    .option('domain', {
      alias: 'd',
      type: 'string',
      description:
        'The domain for which to generate the ontology (e.g., "biology", "space_exploration")',
      demandOption: true,
    })
    .option('instructions', {
      alias: 'i',
      type: 'string',
      description:
        'Specific instructions for the content, source material, or style of the ontology',
      demandOption: true,
    })
    .option('provider', {
      alias: 'p',
      type: 'string',
      description:
        'LLM provider to use (e.g., openai, gemini, ollama). Overrides .env MCR_LLM_PROVIDER.',
      default: process.env.MCR_LLM_PROVIDER || 'ollama',
    })
    .option('model', {
      alias: 'm',
      type: 'string',
      description:
        'Specific model name for the provider. Overrides .env model.',
    })
    .help().argv;

  generateOntology(
    argv.domain,
    argv.instructions,
    argv.provider,
    argv.model
  ).catch((error) => {
    logger.error(
      `An error occurred during ontology generation: ${error.message}`
    );
    process.exit(1);
  });
}
