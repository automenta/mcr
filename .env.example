```env
# This is an example .env file for the Model Context Reasoner (MCR).
# Copy this file to .env and fill in the necessary API keys and configurations.

# --- Server Settings ---
# HOST="0.0.0.0" # Server host. Defaults to 0.0.0.0
# PORT="8080"    # Server port. Defaults to 8080

# --- Logging ---
# LOG_LEVEL="info" # Logging level. Options: error, warn, info, http, verbose, debug, silly. Defaults to info.

# --- LLM Configuration ---
# Choose ONE LLM provider by setting MCR_LLM_PROVIDER and configuring its specific variables.
# Supported values for MCR_LLM_PROVIDER: "ollama", "gemini", "nullllm" (add more as implemented in src/llmService.js)
# MCR_LLM_PROVIDER="ollama" # Default if not set

# For Null LLM Provider (for testing without a live LLM, or if other LLMs are unavailable)
# MCR_LLM_PROVIDER="nullllm"
# No other configuration needed for nullllm. It returns placeholder responses.

# For Ollama (local LLM, default provider)
# No API key needed.
# MCR_LLM_MODEL_OLLAMA="llama3" # Optional, defaults to llama3
# MCR_LLM_OLLAMA_BASE_URL="http://localhost:11434" # Optional, defaults to http://localhost:11434

# For Google Gemini
# MCR_LLM_PROVIDER="gemini"
# GEMINI_API_KEY="..." # REQUIRED if MCR_LLM_PROVIDER is "gemini"
# MCR_LLM_MODEL_GEMINI="gemini-pro" # Optional, defaults to gemini-pro

# --- Placeholder Examples for Other LLM Providers (implement in src/llmService.js and src/config.js to use) ---
# For OpenAI
# MCR_LLM_PROVIDER="openai"
# OPENAI_API_KEY="sk-..." # REQUIRED if MCR_LLM_PROVIDER is "openai"
# MCR_LLM_MODEL_OPENAI="gpt-4o" # Optional, example default for OpenAI

# For Anthropic
# MCR_LLM_PROVIDER="anthropic"
# ANTHROPIC_API_KEY="sk-ant-..." # REQUIRED if MCR_LLM_PROVIDER is "anthropic"
# MCR_LLM_MODEL_ANTHROPIC="claude-3-opus-20240229" # Optional, example default for Anthropic

# For Generic OpenAI-compatible API (e.g. local LLM with OpenAI endpoint, custom proxy)
# MCR_LLM_PROVIDER="generic_openai"
# MCR_LLM_MODEL_GENERIC_OPENAI="your-model-name" # REQUIRED: Specify the model name
# MCR_LLM_GENERIC_OPENAI_BASE_URL="http://localhost:8000/v1" # REQUIRED: Base URL of the API
# MCR_LLM_GENERIC_OPENAI_API_KEY="your_api_key_if_needed" # Optional: API key

# --- Reasoner Configuration ---
# MCR_REASONER_PROVIDER="prolog" # Default if not set. Currently only "prolog" is supported.
# No specific environment variables needed for the default embedded Prolog reasoner.

# --- Ontology Configuration ---
# MCR_ONTOLOGY_DIR="./ontologies" # Optional, defaults to <project_root>/ontologies

# --- Translation Strategy Configuration ---
# MCR_TRANSLATION_STRATEGY="SIR-R1" # Choose the default translation strategy. Options: "Direct-S1", "SIR-R1". Defaults to "SIR-R1".

# --- Debugging Configuration ---
# MCR_DEBUG_LEVEL="none" # Controls verbosity of debug information in API responses.
                         # Options: "none" (minimal, default), "basic" (essential debug fields), "verbose" (full details).

# --- Data Storage (from old example, not currently in new config.js but good for future reference) ---
# Default paths used by old version were ./sessions_data and ./ontologies_data
# These are not actively used by the current src/sessionManager.js or src/ontologyService.js (which uses MCR_ONTOLOGY_DIR)
# MCR_SESSION_STORAGE_PATH="./sessions_data" # Example for session persistence if re-implemented

# --- Neuro-Symbolic Enhancements ---
# REASONER_TYPE="ltn" # Use 'ltn' for Logic Tensor Networks. Defaults to 'prolog'.
# LTN_THRESHOLD="0.7" # Probability cutoff for LTN reasoning.
# EMBEDDING_MODEL="all-MiniLM-L6-v2" # Sentence-transformer model for embeddings.
# KG_ENABLED="true" # Enable the knowledge graph.
```
