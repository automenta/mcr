# This is an example .env file for the Model Context Reasoner (MCR).
# Copy this file to .env and fill in the necessary API keys and configurations.

# --- CHOOSE ONE LLM PROVIDER ---
# Uncomment and set the appropriate variables for your chosen provider.

# For OpenAI
# OPENAI_API_KEY="sk-..."
# MCR_LLM_MODEL_OPENAI="gpt-4o" # Optional, defaults to gpt-4o

# For Google Gemini
# GEMINI_API_KEY="..."
# MCR_LLM_MODEL_GEMINI="gemini-2.5-flash" # Optional, defaults to gemini-pro

# For local Ollama (no API key needed, but set the model if not using the default)
# MCR_LLM_PROVIDER="ollama"
# MCR_LLM_MODEL_OLLAMA="llama3" # Optional, defaults to llama3
# MCR_LLM_OLLAMA_BASE_URL="http://localhost:11434" # Optional, defaults to http://localhost:11434

# --- OPTIONAL GENERAL SETTINGS ---
# MCR_API_URL="http://localhost:8080" # For CLI: defaults to http://localhost:8080 if server is local
# HOST="0.0.0.0" # Server host. Defaults to 0.0.0.0
# PORT="8080"    # Server port. Defaults to 8080
# LOG_LEVEL="info" # Logging level. Options: error, warn, info, http, verbose, debug, silly. Defaults to info.

# --- DATA STORAGE (Optional) ---
# Default paths are ./sessions_data and ./ontologies_data
# SESSIONS_DIR="./sessions_data"
# ONTOLOGIES_DIR="./ontologies_data"
